{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f835dc5-e3f3-4075-8e07-d805753846f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Dropout, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1004\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cf8cfd-ea58-406e-95b2-3043e219cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, infer=False):\n",
    "    def _parse_image(img_path):\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [384, 384])\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = img / 255.0  # Normalize image to range [0, 1]\n",
    "        return img\n",
    "\n",
    "    def _parse_mask(mask_path):\n",
    "        mask = tf.io.read_file(mask_path)\n",
    "        mask = tf.image.decode_png(mask, channels=1)\n",
    "        mask = tf.image.resize(mask, [384, 384])\n",
    "        mask = mask / 255.0  # Normalize mask to range [0, 1]\n",
    "        return mask\n",
    "\n",
    "    img_dataset = tf.data.Dataset.from_tensor_slices(df['img_path'].values)\n",
    "    img_dataset = img_dataset.map(_parse_image)\n",
    "\n",
    "    if not infer:\n",
    "        mask_dataset = tf.data.Dataset.from_tensor_slices(df['mask_path'].values)\n",
    "        mask_dataset = mask_dataset.map(_parse_mask)\n",
    "        dataset = tf.data.Dataset.zip((img_dataset, mask_dataset))\n",
    "    else:\n",
    "        dataset = img_dataset\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c80309f-d2cf-42bd-9543-e41036fdff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(image, mask):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    mask = tf.image.random_flip_left_right(mask)\n",
    "    mask = tf.image.random_flip_up_down(mask)\n",
    "    return image, mask\n",
    "\n",
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000, batch_size=32):\n",
    "    # 캐싱을 사용하여 데이터셋을 로딩 속도를 높입니다.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # 이미지 증강을 수행합니다.\n",
    "    ds = ds.map(augment_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # 배치 크기에 맞게 데이터를 분할하고 데이터를 지속적으로 전달할 수 있게 반복합니다.\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    # 데이터를 prefetch 하여 학습 도중 다음 배치의 데이터를 준비합니다.\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae65704-b386-49d0-bb2d-faee8ed674da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 생성합니다.\n",
    "df = pd.read_csv('image_mask_paths.csv')\n",
    "dataset = create_dataset(df)\n",
    "\n",
    "# 데이터셋의 전체 크기를 구하고 훈련/검증 데이터셋으로 분할합니다.\n",
    "dataset_size = len(df)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "dataset = dataset.shuffle(dataset_size)\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 데이터 증강을 수행하고 배치 단위로 분할합니다.\n",
    "train_dataset = prepare_for_training(train_dataset, batch_size=batch_size)\n",
    "val_dataset = prepare_for_training(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd0e85a-5fb9-4796-959e-5d590a699522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(input_shape=(384, 384, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Contraction path\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # Expansion path\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b45bea8-cacf-4d8f-b0b6-a4851921ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e219c-bb2f-47aa-9a00-845b73f48670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=['/gpu:0', '/gpu:1', '/gpu:2'])\n",
    "with strategy.scope():\n",
    "    model = UNet()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[dice_coef])\n",
    "\n",
    "    chk_point = tf.keras.callbacks.ModelCheckpoint('tf_unet.h5', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(patience=5)\n",
    "    early = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(train_dataset,\n",
    "                        validation_data=val_dataset,\n",
    "                        epochs=100,\n",
    "                        steps_per_epoch=train_size//batch_size,\n",
    "                        validation_steps=val_size//batch_size,\n",
    "                        callbacks=[chk_point, reduce_lr, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a17b3-77b9-4f1d-a889-ee7ff8306577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
